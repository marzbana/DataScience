{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient decent\n",
    "\n",
    "## Exact Gradient Computation\n",
    "\n",
    "Given a function f, we can compute its exact gradient at any x if f's derivative is easy to compute. For example, let $f(x)=2x^2-3x+\\ln x$, where $x>0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Gradient Computation\n",
    "\n",
    "Instead of computing the derivative of a function, we can also estimate the gradient numerically with various methods. These methods are essential, especially when a callable function is not exposed due to privacy reasons, or it is hard to differentiate analytically. \n",
    "\n",
    "To numerically compute the gradient, the simple way is to follow Newton's difference quotient: $f'(x)=\\lim_{h\\to 0}{f(x+h)-f(x)\\over h}$. Another two-point formula is to compute the slope through the points $(x-h,f(x-h))$ and $(x+h,f(x+h))$. Let us reuse the example function $f(x)=2x^2-3x+\\ln x$ and test the precision of these two approaches. We define the function in the next cell, and try to compute its gradient via both methods at $x=2$. Range h value in [0.1,0.01,0.001,0.0001] and report all gradients calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def f(x):\n",
    "    return (2*(x**2) - (3*x) + math.log(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient using the first method (Newton's difference quotient)\n",
      "h=0.1; f'(2)=5.687901641694317\n",
      "h=0.01; f'(2)=5.518754151103744\n",
      "h=0.001; f'(2)=5.50187504165045\n",
      "h=0.0001; f'(2)=5.5001875004201395\n",
      "h=1e-05; f'(2)=5.500018749993173\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient using the first method (Newton's difference quotient)\n",
    "def grad_1(x, h):\n",
    "    return (f(x+h) - f(x)) / h\n",
    "#print gradients from h in [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "print (\"Gradient using the first method (Newton's difference quotient)\")\n",
    "for h in [0.1, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "    print(\"h=\"+str(h)+\"; f'(2)=\"+str(grad_1(2, h)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient using the second method (Two-point difference quotient)\n",
      "h=0.1; f'(2)=5.500417292784909\n",
      "h=0.01; f'(2)=5.500004166729067\n",
      "h=0.001; f'(2)=5.500000041665842\n",
      "h=0.0001; f'(2)=5.500000000417948\n",
      "h=1e-05; f'(2)=5.499999999991622\n"
     ]
    }
   ],
   "source": [
    "# Compute gradient using the second method \n",
    "def grad_2(x, h):\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "#print gradients from h in [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "print (\"Gradient using the second method (Two-point difference quotient)\")\n",
    "for h in [0.1, 0.01, 0.001, 0.0001, 0.00001]:\n",
    "    print(\"h=\"+str(h)+\"; f'(2)=\"+str(grad_2(2, h)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the second method is more accurate for the given equation because it was very close to the actual derivative at $x=2$ even with the first value of h. However, more trials with more equations and different values of h would be needed to definitively say which one is more accurate in general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "Logistic regression is a classification tool that models the probability of an event taking place by having the log odds for the event be a linear combination of one or more independent variables. Specifically, let $\\vec{x}=<x_1,\\dots ,x_m>$ be an m dimensional vector of independent variables (features), and $y$ be the corresponding binary dependent variable (label). The probability of having $y=1$ is modeled as $$P_y={1\\over 1+e^{-(b_0+b_1\\cdot x_1+\\dots +b_m\\cdot x_m)}}={1\\over 1+e^{-(b_0+\\vec{b}_{1:m}\\cdot\\vec{x})}}$$\n",
    "\n",
    "Given a set of data points $<\\vec{x}_k,y_k>$ with $k\\in [1,n]$, how can we fit the model with these data, i.e., how to choose the best $\\vec{b}=b_0,b_1\\cdots,b_m$?\n",
    "\n",
    "One way is to write out the likelihood $$\\prod_{k:y_k=1}P_{y_k}\\prod_{k:y_k=0}(1-P_{y_k})$$ and find $b_0,b_1\\cdots,b_m$ that maximize its logarithm, $$l=\\sum_{k:y_k=1}\\ln(P_{y_k})+\\sum_{k:y_k=0}\\ln(1-P_{y_k})$$\n",
    "\n",
    "In contrast to computing the closed form gradient of a Least-squares loss in a linear model, doing the same for logistic regression is not possible. Gradient descent can be used to optimize such function $l$, and we will implement it step-by-step. First, we write a function log_likelihood in the next cell that computes the log-likelihood given data points and $\\vec{b}$. [5 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(X,y,b):\n",
    "    \"\"\"\n",
    "    X: n*m numpy data array.\n",
    "    y: one dimension numpy data array of length n\n",
    "    b: one dimension numpy data array of length m+1\n",
    "    \n",
    "    Return the log likelihood.\n",
    "    \"\"\"\n",
    "    #commpute P_yk\n",
    "    p = ([[0, 0] for i in range(len(y)) ])\n",
    "    for i in range(len(y)):\n",
    "        p[i][0] = 1/(1+np.exp(-1*(b[0]+np.dot(X[i],b[1:]))))\n",
    "        p[i][1] = 1-p[i][0]\n",
    "    #compute log likelihood\n",
    "    log_likelihood = 0\n",
    "    for i in range(len(y)):\n",
    "        if y[i] == 1:\n",
    "            log_likelihood += np.log(p[i][0])\n",
    "        else:\n",
    "            log_likelihood += np.log(p[i][1])\n",
    "    return log_likelihood\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X,y,b):\n",
    "# The inputs are the same as the ones of log_likelihood\n",
    "# Return the gradient of the log likelihood with respect to vector b\n",
    "    def f(x):\n",
    "        return log_likelihood(X,y,x)\n",
    "    def grad_1(x, h):\n",
    "        grad=[0 for _ in range(len(x))]\n",
    "        for i in range(len(x)):\n",
    "            h2=[0 for _ in range(len(x))]\n",
    "            h2[i]=h \n",
    "            grad[i]=(f(x+h2)-f(x))/h\n",
    "        return np.array(grad)\n",
    "    return grad_1(b, 0.00001)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8785005 , -0.09478745])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.optimize as opt\n",
    "# Test your function here, preserve the output\n",
    "compute_gradient(X,y,b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we know how to compute the gradients, we can optimize the objective, which is log-likelihood in our case, using gradient descent. It iteratively changes the parameters in a small \"step\" towards the gradient direction, i.e., the direction where the objective increases at the fastest pace. Formally, denote the calculated gradients as $\\Delta (\\vec{b})$, we can update our parameters via $\\vec{b}=\\vec{b}+\\gamma \\cdot \\Delta (\\vec{b})$, where $\\gamma$ is the size of the \"step\". We repeat this process until the objective stop improving or a pre-set max number of iterations is reached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, initial_b, step_size, max_iteration,prin=False):\n",
    "    \"\"\"\n",
    "    X: n*m numpy data array.\n",
    "    y: one dimension numpy data array of length n\n",
    "    initial_b: one dimension numpy data array of length m+1\n",
    "    step_size: scalar, the size of one step update\n",
    "    max_iteration: scalar, the max number of iterations\n",
    "    Return the updated coefficient vector b.\n",
    "    \"\"\"\n",
    "    b = initial_b\n",
    "    c=max_iteration\n",
    "    for i in range(max_iteration):\n",
    "        grad=compute_gradient(X,y,b)\n",
    "        length=np.linalg.norm(grad)\n",
    "        if length > 1e-8:\n",
    "            b=b+(step_size*grad/length)\n",
    "        else: \n",
    "            c=i\n",
    "            break\n",
    "    if(prin):\n",
    "        print(\"final number of iterations: \"+str(c))\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-52.42477565  70.06060629]\n"
     ]
    }
   ],
   "source": [
    "optimized_b = gradient_descent(X, y, b, 0.1, 1000)\n",
    "print(optimized_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we apply the implemented logistic regression model to a real dataset. The dataset is a trimmed breast-cancer-Wisconsin dataset from UCI machine learning Repository. Only 100 data points are offered in the training set to make sure the computation can be finished swiftly. The training dataset is loaded in the next cell, and the vector $\\vec{b}$ is also randomly initialized. \n",
    "\n",
    "Fit three models with the training set using different step size ranging in [0.01,0.05,0.1] and set the max number of iterations as 10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"breast-cancer-wisconsin.data\",\"r\")\n",
    "X_train = []\n",
    "y_train = []\n",
    "for line in f:\n",
    "    tmp = []\n",
    "    for part in line.strip().split(\",\")[1:-1]:\n",
    "        tmp.append(float(part))\n",
    "    y_train.append((0 if line.strip().split(\",\")[-1]==\"2\" else 1))\n",
    "    X_train.append(tmp)\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "random_b = np.random.uniform(0,1,size=(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step size: 0.01\n",
      "final number of iterations: 10000\n",
      "final log-likelihood: -7.187028615249007\n",
      "\n",
      "step size: 0.05\n",
      "final number of iterations: 10000\n",
      "final log-likelihood: -7.247861048924458\n",
      "\n",
      "step size: 0.1\n",
      "final number of iterations: 10000\n",
      "final log-likelihood: -7.626632940942781\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit three models with different step size, report the final log-likelihood, \n",
    "# number of iterations and the final coefficent vector b.\n",
    "b=[0,0,0]\n",
    "i=0\n",
    "for h in (.01, .05, .1):\n",
    "    print(\"step size: \"+str(h))\n",
    "    b[i]=gradient_descent(X_train, y_train, random_b, h, 10000, prin=True)\n",
    "    print(\"final log-likelihood: \"+str(log_likelihood(X_train, y_train, b[i]))+\"\\n\")\n",
    "    i+=1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final log-likelihoods got smaller as the step size increased in length. However, the number of iterations remained the same (the max value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"test_data.txt\",\"r\")\n",
    "X_test = []\n",
    "y_test = []\n",
    "for line in f:\n",
    "    tmp = []\n",
    "    for part in line.strip().split(\",\")[1:-1]:\n",
    "        tmp.append(float(part))\n",
    "    y_test.append((0 if line.strip().split(\",\")[-1]==\"2\" else 1))\n",
    "    X_test.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for step size .01: 0.9\n",
      "accuracy for step size .05: 0.9\n",
      "accuracy for step size .1: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Predict based on your models and report the accuracy\n",
    "results=[[0,0,0] for _ in range(len(y_test))]\n",
    "prediction=[[0,0,0] for _ in range(len(y_test))]\n",
    "for i in range(len(results)):\n",
    "    results[i][0]= 1/(1+np.exp(-1*(b[0][0]+np.dot(X_test[i],b[0][1:]))))\n",
    "    results[i][1]= 1/(1+np.exp(-1*(b[1][0]+np.dot(X_test[i],b[1][1:]))))\n",
    "    results[i][2]= 1/(1+np.exp(-1*(b[2][0]+np.dot(X_test[i],b[2][1:]))))\n",
    "    prediction[i][0]=1 if (results[i][0]) >.5 else 0\n",
    "    prediction[i][1]=1 if (results[i][1]) >.5 else 0\n",
    "    prediction[i][2]=1 if (results[i][2]) >.5 else 0\n",
    "accuracy=[0,0,0]\n",
    "for i in range(len(y_test)):\n",
    "    if prediction[i][0]==y_test[i]:\n",
    "        accuracy[0]+=1\n",
    "    if prediction[i][1]==y_test[i]:\n",
    "        accuracy[1]+=1\n",
    "    if prediction[i][2]==y_test[i]:\n",
    "        accuracy[2]+=1\n",
    "accuracy[0]/=len(y_test)\n",
    "accuracy[1]/=len(y_test)\n",
    "accuracy[2]/=len(y_test)\n",
    "print(\"accuracy for step size .01: \"+str(accuracy[0]))\n",
    "print(\"accuracy for step size .05: \"+str(accuracy[1]))\n",
    "print(\"accuracy for step size .1: \"+str(accuracy[2]))\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
