{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Gaussian MLE\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "We have $n=100$ pieces of indepdendent and identicially distrubuted (i.i.d) data related to some measurement that is drawn from a **normal distribution**.\n",
    "\n",
    "We will write code that computes $\\mu_{MLE}$ and $\\sigma_{MLE}$, which represent the mean and standard deviation computed via MLE, respectively, of the underlying normal distribution for the dataset \"MLE_dataset.npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102.65639598061982, 9.743253619192298)\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def load(path):\n",
    "    '''\n",
    "    loads \"MLE_dataset.npy\" given its path into variable 'dataset'. returns 'dataset'\n",
    "    '''\n",
    "    dataset = np.load(path)\n",
    "    return dataset\n",
    "    \n",
    "def MLE(dataset):\n",
    "    '''\n",
    "    Input:\n",
    "        dataset - numpy array of shape (100,) - containing the data drawn from unknown gaussian\n",
    "        \n",
    "    Output:\n",
    "        mu - float - MLE estimate of mu based on data\n",
    "        sigma - float - MLE estimate of sigma based on data\n",
    "    '''\n",
    "    mu = 0\n",
    "    sigma = 0\n",
    "    for i in range(dataset.size):\n",
    "        mu += dataset[i]\n",
    "    mu = mu/dataset.size\n",
    "    for i in range(dataset.size):\n",
    "        sigma += np.power(dataset[i]-mu, 2)\n",
    "    sigma = sigma/dataset.size\n",
    "    sigma = np.power(sigma, 1/2)\n",
    "    return mu, sigma\n",
    "        \n",
    "#load dataset. assumes data file is in same directory as code file\n",
    "dataset = load(\"./MLE_dataset.npy\")\n",
    "mu_sigma = MLE(dataset)\n",
    "print (mu_sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Gaussian EM\n",
    "\n",
    "## Problem statement\n",
    "\n",
    "We will add a layer of difficulty to the problem of estimating the underlying distribution(s) of our data. Previously. we knew that all of our data came from a single underlying normal distribution.\n",
    "\n",
    "For this problem, we have a brand new data set consisting of $n=200$ data points. These data points are drawn from **one of two unknown gaussian distributions**.\n",
    "\n",
    "To keep the notation consistent, we will use the subscript ID $k = \\{0,1\\}$ to represent which gaussian a parameter/variable is referring to.\n",
    "\n",
    "For instance, $\\mu_k$ represents the mean of the gaussian with ID $k$. $\\mu_0$ represents the mean of the first gaussian, and $\\mu_1$ represents the mean of the second gaussian\n",
    "\n",
    "## Variable definitions\n",
    "\n",
    "1. $k$: id of the gaussian distributions. {0,1} \n",
    "2. $n$: number of data points. 200 in this case\n",
    "3. $\\mu_k$: the mean of normal distribution $k$\n",
    "4. $\\sigma_k$: the std of normal distribution $k$\n",
    "5. $\\pi_k$: the prior probability of normal distribution $k$\n",
    "7. $t$: current step number\n",
    "8. $z_i = k$: represents the idea that i-th data point was drawn from gaussian $k$\n",
    "9. $x_i$: i-th data point\n",
    "\n",
    "### Task Order\n",
    "1. Determine the update rules for $\\mu_k^{t+1}$ and $\\sigma_k^{t+1}$ and write them in the cell below\n",
    "2. Code the E and M step\n",
    "3. Run and test your code on \"EM_dataset.py\"\n",
    "\n",
    "\n",
    "## E - Step (8 pts)\n",
    "$P(z_i = k | x_i, \\theta_k^t)$ reflects the responsibility the k-th gaussian has for the i-th data point\n",
    "$P(z_i = k | x_i, \\theta_k^t) = \\frac{P(x_i | z_i = k, \\theta_k^t)*\\pi_k^t}{P(x_i | \\theta_k^t)}$\n",
    "$= \\frac{P(x_i | z_i = k, \\theta_k^t)*\\pi_k^t}{\\sum_{k=0}^{1}P(x_i | z_i = k, \\theta_k^t)*\\pi_k^t}$\n",
    "\n",
    "\n",
    "\n",
    "## M - Step (10 pts)\n",
    "\n",
    "\n",
    "$$\\mu_k^{t+1} = \\ ??$$\n",
    "\n",
    "$$\\sigma_k^{t+1} = \\ ??$$\n",
    "\n",
    "$$\\pi_k^{t+1} = \\frac{\\sum_{i=1}^n P(z_i = k | x_i, \\theta_k^t)}{n}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulas\n",
    "\n",
    "\n",
    "$\\mu_k^{t+1} = \\frac{\\sum_{i=1}^n P(z_i = k | x_i, \\theta_k^t)*x_i}{N_k^{t}}$\n",
    "\n",
    "\n",
    "$\\sigma_k^{t+1}=\\sqrt{\\frac{\\sum_{i=1}^n P(z_i = k | x_i, \\theta_k^t)*(x_i-\\mu_k^{k+1})^2}{N_k^{t}}}$\n",
    "\n",
    "$N_k^{t}=\\sum_{i=1}^n P(z_i = k | x_i, \\theta_k^t)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pis:\n",
      "[0.41534395 0.58465605]\n",
      "Mus:\n",
      "[100.50444169 201.90594025]\n",
      "Sigmas:\n",
      "[23.10763968 19.8353594 ]\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "\n",
    "def load(path):\n",
    "    dataset = np.load(path)\n",
    "    return dataset\n",
    "\n",
    "def em(dataset, k, n_iterations):\n",
    "    '''\n",
    "    Input:\n",
    "        dataset - np array - containing the data\n",
    "        k - int - representing the number of underlying gaussian distributions\n",
    "        n_iterations - int - representing number of iterations EM should run for\n",
    "        \n",
    "    output:\n",
    "        mus - np array shape (2,) - mus[k] is the EM estimate of the mean of the kth gaussian\n",
    "        sigmas - np array shape (2,) - sigmas[k] is the EM estimate of the stdev of the kth gaussian\n",
    "        pi - np array shape (2,) - pis[j] is the EM estimate of the prior of the kth gaussian\n",
    "    '''\n",
    "    n_samples = dataset.shape[0]\n",
    "\n",
    "    # Initial guesses for the parameters DO NOT CHANGE\n",
    "    FINAL_INITIAL_MUS = np.asarray([90.0, 210.0]) #DO NOT CHANGE\n",
    "    FINAL_INITIAL_SIGMAS = np.asarray([28.0,19.0]) #DO NOT CHANGE\n",
    "    FINAL_INITIAL_PIS = np.asarray([0.3,.7]) #DO NOT CHANGE\n",
    "    pis = FINAL_INITIAL_PIS #DO NOT CHANGE\n",
    "    mus = FINAL_INITIAL_MUS #DO NOT CHANGE\n",
    "    sigmas = FINAL_INITIAL_SIGMAS #DO NOT CHANGE\n",
    "    \n",
    "    for em_iter in (range(n_iterations)):\n",
    "            \n",
    "            \n",
    "            \n",
    "            #E Step\n",
    "            #data\n",
    "            #store Pr(zi|xi, data) for each iteration\n",
    "            #commputed in e step, used in m step\n",
    "            d= [[0.0]*k for i in range(n_samples)]\n",
    "            #Pr(xi|zi, data) store\n",
    "            #used in e step\n",
    "            c=[[0.0] for i in range(k)]\n",
    "            #denominator of pr(zi|xi, data)\n",
    "            #used in e step\n",
    "            denom=0\n",
    "            #store Pr(zi|xi, data)*xi\n",
    "            \n",
    "            #calculating Pr(zi|xi, data)\n",
    "            for i in range(n_samples):\n",
    "                #setting denom to 0 for the next iteration\n",
    "                denom=0\n",
    "                #calculating Pr(xi|zi, data)\n",
    "                for a in range(k):\n",
    "                    c[a]=scipy.stats.norm(mus[a], sigmas[a]).pdf(dataset[i]) \n",
    "                #adding all Pr(xi|zi, data)*pis for denom and \n",
    "                #setting numerator \n",
    "                for a in range(k):\n",
    "                    d[i][a]=c[a]*pis[a]\n",
    "                    denom += d[i][a]\n",
    "                #dividing pr(xi|zi, data)*pis by summation of \n",
    "                #pr(xi|zi, data)*pis   \n",
    "                for a in range(k):\n",
    "                    d[i][a]=d[i][a]/denom\n",
    "\n",
    "            #M step\n",
    "            #only m step data\n",
    "            #commputed in m step, used in m step\n",
    "            x= [[0.0]*k for i in range(n_samples)]\n",
    "            #store ni: the approximation of the number of \n",
    "            #rvs for each distribution \n",
    "            #used to find mus and sigmas every iteration\n",
    "            #used in m step and commputed in it\n",
    "            size=[[0.0] for i in range(k)]\n",
    "            \n",
    "            #calculating the expected data for dist. 1 and 2\n",
    "            #summation Pr(zi|xi, data)*xi\n",
    "            #nummerator of mus\n",
    "            for i in range(n_samples):\n",
    "                #iterating over each distribution\n",
    "                for a in range(k):\n",
    "                    x[i][a]=dataset[i]*d[i][a]\n",
    "            \n",
    "            #setting pis to 0 so we can find the next \n",
    "            #rounds approximations\n",
    "            for i in range(k):\n",
    "                pis[i]=0\n",
    "\n",
    "            #adding all pr(zi|xi, data) for each pis\n",
    "            for i in range(n_samples):\n",
    "                #iterating over #distributions\n",
    "                for a in range(k):\n",
    "                    pis[a] += d[i][a]\n",
    "                \n",
    "            #dividing by n\n",
    "            for i in range(k):\n",
    "                pis[i] = pis[i]/n_samples\n",
    "            #calculating the approximated number of samples \n",
    "            #from each distribution\n",
    "            #because pis now has the current data multiplyig \n",
    "            #by n_samples now leaves us \n",
    "            #the summation of all Pr(zi|xi, data)\n",
    "            for i in range(k):\n",
    "                size[i]=n_samples*pis[i]\n",
    "            #setting mus to zero to caculate next rounds\n",
    "            for i in range(k):\n",
    "                mus[i]=0\n",
    "            #adding Pr(zi|xi, data)*xi for all samples\n",
    "            for i in range(n_samples):\n",
    "                #itertating over each distribution\n",
    "                for a in range(k):\n",
    "                    mus[a] += x[i][a]\n",
    "            #dividing summation Pr(zi|xi, data)*xi \n",
    "            #by the aprroximated sizes\n",
    "            for i in range(k):\n",
    "                mus[i]=mus[i]/size[i]\n",
    "            #setting sigmas to 0 for new calculation\n",
    "            for i in range(k):\n",
    "                sigmas[i]=0\n",
    "            #adding Pr(zi|xi, data)*(xi-mus)^2 for each sigmas\n",
    "            for i in range(n_samples):\n",
    "                #iterating over each distribution\n",
    "                for a in range(k):\n",
    "                    sigmas[a] +=d[i][a]* np.power((dataset[i]-mus[a]), 2)\n",
    "            #dividing by approximated size and taking \n",
    "            #square root to find sigmas instead of sigmas^2\n",
    "            for i in range(k):\n",
    "                sigmas[i] = sigmas[i]/size[i]\n",
    "                sigmas[i] = np.power(sigmas[i], 1/2)\n",
    "            \n",
    "            \n",
    "    return pis, mus, sigmas\n",
    "\n",
    "def main():\n",
    "    n_iterations = 20 #DO NOT CHANGE\n",
    "    k = 2 #DO NOT CHANGE\n",
    "    #load dataset. assumes data file is in \n",
    "    #same directory as code file\n",
    "    dataset = np.load(\"EM_dataset.npy\")\n",
    "    pis, mus, sigmas = em(dataset, k, n_iterations)\n",
    "    print(\"Pis:\")\n",
    "    print(pis)\n",
    "    print(\"Mus:\")\n",
    "    print(mus)\n",
    "    print(\"Sigmas:\")\n",
    "    print(sigmas)\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
